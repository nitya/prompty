# Serverless Example

Prompty can be configured to use serverless models.

Refer to the [serverless.prompty](#filepath: /workspaces/prompty/runtime/promptycs/Prompty.Core.Tests/prompty/serverless.prompty) file for more examples.

```yaml
---
name: Serverless Example
description: A prompt that uses a serverless model
authors:
  - example_author
model:
  api: chat
  configuration:
    type: serverless
    endpoint: https://models.inference.ai.azure.com
    model: Mistral-small
    key: ${env:SERVERLESS_KEY:KEY}
sample:
  question: What is the weather like today?
---
system:
You are a helpful assistant.
user:
{{question}}
```
